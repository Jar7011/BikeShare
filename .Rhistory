## Cleaning and organizing data ##
# Clean and tidy up training data
bike_training_data <- bike_training_data |>
select(-casual, -registered)
bike_training_data$season <- factor(bike_training_data$season,
levels = c(1,2,3,4),
labels = c('Spring', 'Summer',
'Fall', 'Winter'))
bike_training_data$weather <- factor(bike_training_data$weather,
levels = c(1,2,3,4))
bike_training_data <- bike_training_data |>
mutate(weather = ifelse(weather == 4, 3, weather))
# Clean and tidy up test data to match training data format
bike_test_data$season <- factor(bike_test_data$season,
levels = c(1,2,3,4),
labels = c('Spring', 'Summer',
'Fall', 'Winter'))
bike_test_data$weather <- factor(bike_test_data$weather,
levels = c(1,2,3,4))
bike_test_data <- bike_test_data |>
mutate(weather = ifelse(weather == 4, 3, weather))
# Set up poisson regression model
poisson_model <- poisson_reg() |>
set_engine('glm') |>
set_mode('regression') |>
fit(formula=count~., data=bike_training_data)
# Make predictions
pois_predictions <- predict(poisson_model,
new_data = bike_test_data)
# Format the Predictions for Submission to Kaggle
pois_kaggle_submission <- pois_predictions %>%
bind_cols(., bike_test_data) %>% # Bind predictions with test data
select(datetime, .pred) %>% # Just keep datetime and prediction variables
rename(count = .pred) %>% # Rename to count (for submission to Kaggle)
mutate(datetime = as.character(format(datetime))) # Needed for right format to Kaggle
# Write out the file
vroom_write(x = pois_kaggle_submission, file = "./Poisson_Predictions.csv", delim = ",")
# Load necessary libraries
library(tidyverse)
library(tidymodels)
library(DataExplorer)
library(vroom)
library(patchwork)
library(poissonreg)
## Linear model with recipe and workflow ##
# Read in training data again to reset file
bike_training_data_2 <- vroom('train.csv')
# Read in test data again to reset file
bike_test_data_2 <- vroom('test.csv')
# Eliminate casual and registered columns from training data and
#change count to log(count)
bike_training_data_2 <- bike_training_data_2 |>
select(-casual, -registered) |>
mutate(count = log(count))
# Create recipe
bike_recipe <- recipe(count ~ ., data=bike_training_data_2) |>
step_mutate(weather = ifelse(weather == 4, 3, weather)) |>
step_mutate(weather = factor(weather, levels = c(1,2,3))) |>
step_time(datetime, features = 'hour') |>
step_mutate(season = factor(season,
levels = c(1,2,3,4),
labels = c('Spring', 'Summer',
'Fall', 'Winter'))) |>
step_rm(atemp)
# Define model
lin_model <- linear_reg() |>
set_engine('lm') |>
set_mode('regression')
# Combine into workflow
bike_workflow <- workflow() |>
add_recipe(bike_recipe) |>
add_model(lin_model) |>
fit(data = bike_training_data_2)
# Get predictions
lin_preds <- predict(bike_workflow, new_data = bike_test_data_2) |>
mutate(.pred = exp(.pred))
# Format the Predictions for Submission to Kaggle
kaggle_submission <- lin_preds %>%
bind_cols(., bike_test_data_2) %>% # Bind predictions with test data
select(datetime, .pred) %>% # Just keep datetime and prediction variables
rename(count = .pred) %>% # Rename to count (for submission to Kaggle)
mutate(count = pmax(0, count)) %>% # Pointwise max of (0, prediction)
mutate(datetime = as.character(format(datetime))) # Needed for right format to Kaggle
# Write out the file
vroom_write(x = kaggle_submission, file = "./Linear_Predictions_Workflow.csv", delim = ",")
# Load necessary libraries
library(tidyverse)
library(tidymodels)
library(DataExplorer)
library(vroom)
library(patchwork)
library(poissonreg)
## Linear model with recipe and workflow ##
# Read in training data again to reset file
bike_training_data_2 <- vroom('train.csv')
# Read in test data again to reset file
bike_test_data_2 <- vroom('test.csv')
# Eliminate casual and registered columns from training data and
#change count to log(count)
bike_training_data_2 <- bike_training_data_2 |>
select(-casual, -registered) |>
mutate(count = log(count))
# Create recipe
bike_recipe <- recipe(count ~ ., data=bike_training_data_2) |>
step_mutate(weather = ifelse(weather == 4, 3, weather)) |>
step_mutate(weather = factor(weather, levels = c(1,2,3))) |>
step_time(datetime, features = 'hour') |>
step_mutate(season = factor(season,
levels = c(1,2,3,4),
labels = c('Spring', 'Summer',
'Fall', 'Winter'))) |>
step_rm(atemp)
# Define model
lin_model <- linear_reg() |>
set_engine('lm') |>
set_mode('regression')
# Combine into workflow
bike_workflow <- workflow() |>
add_recipe(bike_recipe) |>
add_model(lin_model) |>
fit(data = bike_training_data_2)
# Get predictions
lin_preds <- predict(bike_workflow, new_data = bike_test_data_2) |>
mutate(.pred = exp(.pred))
# Format the Predictions for Submission to Kaggle
kaggle_submission <- lin_preds %>%
bind_cols(., bike_test_data_2) %>% # Bind predictions with test data
select(datetime, .pred) %>% # Just keep datetime and prediction variables
rename(count = .pred) %>% # Rename to count (for submission to Kaggle)
mutate(count = pmax(0, count)) %>% # Pointwise max of (0, prediction)
mutate(datetime = as.character(format(datetime))) # Needed for right format to Kaggle
# Write out the file
vroom_write(x = kaggle_submission, file = "./Linear_Predictions_Workflow.csv", delim = ",")
# Load necessary libraries
library(tidyverse)
library(tidymodels)
library(DataExplorer)
library(vroom)
library(patchwork)
library(poissonreg)
## Linear model with recipe and workflow ##
# Read in training data again to reset file
bike_training_data <- vroom('train.csv')
# Read in test data again to reset file
bike_test_data <- vroom('test.csv')
# Eliminate casual and registered columns from training data and
#change count to log(count)
bike_training_data <- bike_training_data |>
select(-casual, -registered) |>
mutate(count = log(count))
# Create recipe
bike_recipe <- recipe(count ~ ., data=bike_training_data) |>
step_mutate(weather = ifelse(weather == 4, 3, weather)) |>
step_mutate(weather = factor(weather, levels = c(1,2,3))) |>
step_time(datetime, features = 'hour') |>
step_mutate(season = factor(season,
levels = c(1,2,3,4),
labels = c('Spring', 'Summer',
'Fall', 'Winter'))) |>
step_rm(atemp)
# Define model
lin_model <- linear_reg() |>
set_engine('lm') |>
set_mode('regression')
# Combine into workflow
bike_workflow <- workflow() |>
add_recipe(bike_recipe) |>
add_model(lin_model) |>
fit(data = bike_training_data)
# Get predictions
lin_preds <- predict(bike_workflow, new_data = bike_test_data) |>
mutate(.pred = exp(.pred))
# Format the Predictions for Submission to Kaggle
kaggle_submission <- lin_preds %>%
bind_cols(., bike_test_data) %>% # Bind predictions with test data
select(datetime, .pred) %>% # Just keep datetime and prediction variables
rename(count = .pred) %>% # Rename to count (for submission to Kaggle)
mutate(count = pmax(0, count)) %>% # Pointwise max of (0, prediction)
mutate(datetime = as.character(format(datetime))) # Needed for right format to Kaggle
# Write out the file
vroom_write(x = kaggle_submission, file = "./Linear_Predictions_Workflow.csv", delim = ",")
# Load necessary libraries
library(tidyverse)
library(tidymodels)
library(DataExplorer)
library(vroom)
library(patchwork)
library(poissonreg)
## Penalized regression model ##
# Read in training data again to reset file
bike_training_data <- vroom('train.csv')
# Read in test data again to reset file
bike_test_data <- vroom('test.csv')
# Eliminate casual and registered columns from training data and
#change count to log(count)
bike_training_data <- bike_training_data |>
select(-casual, -registered) |>
mutate(count = log(count))
# Create recipe for penalized regression model
penalized_recipe <- recipe(count ~ ., data=bike_training_data) |>
step_mutate(weather = ifelse(weather == 4, 3, weather)) |>
step_mutate(weather = factor(weather, levels = c(1,2,3))) |>
step_time(datetime, features = 'hour') |>
step_date(datetime, features = 'dow') |>
step_mutate(season = factor(season,
levels = c(1,2,3,4),
labels = c('Spring', 'Summer',
'Fall', 'Winter'))) |>
step_rm(atemp, datetime) |>
step_dummy(all_nominal_predictors()) |>
step_normalize(all_numeric_predictors())
# Create penalized regression model
penalized_model <- linear_reg(penalty = 0.01, mixture = 0.1) |>
set_engine('glmnet')
penalized_workflow <- workflow() |>
add_recipe(penalized_recipe) |>
add_model(penalized_model) |>
fit(data=bike_training_data)
penalized_preds <- predict(penalized_workflow, new_data = bike_test_data) |>
mutate(.pred = exp(.pred))
# Format the Predictions for Submission to Kaggle
kaggle_submission <- penalized_preds %>%
bind_cols(., bike_test_data) %>% # Bind predictions with test data
select(datetime, .pred) %>% # Just keep datetime and prediction variables
rename(count = .pred) %>% # Rename to count (for submission to Kaggle)
mutate(count = pmax(0, count)) %>% # Pointwise max of (0, prediction)
mutate(datetime = as.character(format(datetime))) # Needed for right format to Kaggle
# Write out the file
vroom_write(x = kaggle_submission, file = "./Penalized_Regression.csv", delim = ",")
# Load necessary libraries
library(tidyverse)
library(tidymodels)
library(DataExplorer)
library(vroom)
library(patchwork)
library(poissonreg)
## Tuned Penalized Model ##
# Read in training data
bike_training_data <- vroom('train.csv')
# Read in test data
bike_test_data <- vroom('test.csv')
# Eliminate casual and registered columns from training data and
#change count to log(count)
bike_training_data <- bike_training_data %>%
select(-casual, -registered) %>%
mutate(count = log(count))
# Create recipe for penalized regression model
tuned_penalized_recipe <- recipe(count ~ ., data=bike_training_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(weather = factor(weather, levels = c(1,2,3))) %>%
step_time(datetime, features = 'hour') %>%
step_date(datetime, features = 'dow') %>%
step_mutate(season = factor(season,
levels = c(1,2,3,4),
labels = c('Spring', 'Summer',
'Fall', 'Winter'))) %>%
step_rm(atemp, datetime) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
# Tuned penalized regression model
tuned_penalized_model <- linear_reg(penalty = tune(),
mixture = tune()) %>%
set_engine('glmnet')
# Set Workflow
tuned_penalized_wf <- workflow() %>%
add_recipe(tuned_penalized_recipe) %>%
add_model(tuned_penalized_model)
# Grid of values to tune over
grid_tuning_params <- grid_regular(penalty(),
mixture(),
levels = 5)
# Split data for cross validation (CV)
folds <- vfold_cv(bike_training_data, v = 10, repeats = 1)
# Run the CV
cv_results <- tuned_penalized_wf %>%
tune_grid(resamples = folds,
grid = grid_tuning_params,
metrics = metric_set(rmse, mae, rsq))
# Plot results
collect_metrics(cv_results) %>%
filter(.metric == 'rmse') %>%
ggplot(data=., aes(x=penalty, y=mean, color=factor(mixture))) +
geom_line()
# Find best tuning params
best_params <- cv_results %>%
select_best(metric = 'rmse')
# Finalize workflow and fit it
final_wf <- tuned_penalized_wf %>%
finalize_workflow(best_params) %>%
fit(data = bike_training_data)
# Predict
tuned_penalized_preds <- predict(final_wf, new_data = bike_test_data) %>%
mutate(.pred = exp(.pred))
# Format the Predictions for Submission to Kaggle
kaggle_submission <- tuned_penalized_preds %>%
bind_cols(., bike_test_data) %>% # Bind predictions with test data
select(datetime, .pred) %>% # Just keep datetime and prediction variables
rename(count = .pred) %>% # Rename to count (for submission to Kaggle)
mutate(count = pmax(0, count)) %>% # Pointwise max of (0, prediction)
mutate(datetime = as.character(format(datetime))) # Needed for right format to Kaggle
# Load necessary libraries
library(tidyverse)
library(tidymodels)
library(DataExplorer)
library(vroom)
library(patchwork)
library(poissonreg)
## Tuned Penalized Model ##
# Read in training data
bike_training_data <- vroom('train.csv')
# Read in test data
bike_test_data <- vroom('test.csv')
# Eliminate casual and registered columns from training data and
#change count to log(count)
bike_training_data <- bike_training_data %>%
select(-casual, -registered) %>%
mutate(count = log(count))
# Create recipe for penalized regression model
tuned_penalized_recipe <- recipe(count ~ ., data=bike_training_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(weather = factor(weather, levels = c(1,2,3))) %>%
step_time(datetime, features = 'hour') %>%
step_date(datetime, features = 'dow') %>%
step_mutate(season = factor(season,
levels = c(1,2,3,4),
labels = c('Spring', 'Summer',
'Fall', 'Winter'))) %>%
step_rm(atemp, datetime) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
# Tuned penalized regression model
tuned_penalized_model <- linear_reg(penalty = tune(),
mixture = tune()) %>%
set_engine('glmnet')
# Set Workflow
tuned_penalized_wf <- workflow() %>%
add_recipe(tuned_penalized_recipe) %>%
add_model(tuned_penalized_model)
# Grid of values to tune over
grid_tuning_params <- grid_regular(penalty(),
mixture(),
levels = 5)
# Split data for cross validation (CV)
folds <- vfold_cv(bike_training_data, v = 10, repeats = 1)
# Run the CV
cv_results <- tuned_penalized_wf %>%
tune_grid(resamples = folds,
grid = grid_tuning_params,
metrics = metric_set(rmse, mae, rsq))
# Plot results
collect_metrics(cv_results) %>%
filter(.metric == 'rmse') %>%
ggplot(data=., aes(x=penalty, y=mean, color=factor(mixture))) +
geom_line()
# Find best tuning params
best_params <- cv_results %>%
select_best(metric = 'rmse')
# Finalize workflow and fit it
final_wf <- tuned_penalized_wf %>%
finalize_workflow(best_params) %>%
fit(data = bike_training_data)
# Predict
tuned_penalized_preds <- predict(final_wf, new_data = bike_test_data) %>%
mutate(.pred = exp(.pred))
# Format the Predictions for Submission to Kaggle
kaggle_submission <- tuned_penalized_preds %>%
bind_cols(., bike_test_data) %>% # Bind predictions with test data
select(datetime, .pred) %>% # Just keep datetime and prediction variables
rename(count = .pred) %>% # Rename to count (for submission to Kaggle)
mutate(count = pmax(0, count)) %>% # Pointwise max of (0, prediction)
mutate(datetime = as.character(format(datetime))) # Needed for right format to Kaggle
# Write out the file
vroom_write(x = kaggle_submission, file = "./Tuned_Penalized_Regression.csv", delim = ",")
# Load necessary libraries
library(tidyverse)
library(tidymodels)
library(vroom)
# Eliminate casual and registered columns from training data and
#change count to log(count)
bike_training_data <- bike_training_data %>%
select(-casual, -registered) %>%
mutate(count = log(count))
# Read in training data
bike_training_data <- vroom('train.csv')
# Read in test data
bike_test_data <- vroom('test.csv')
# Eliminate casual and registered columns from training data and
#change count to log(count)
bike_training_data <- bike_training_data %>%
select(-casual, -registered) %>%
mutate(count = log(count))
# Create recipe for regression tree model
tuned_penalized_recipe <- recipe(count ~ ., data=bike_training_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(weather = factor(weather, levels = c(1,2,3))) %>%
step_time(datetime, features = 'hour') %>%
step_date(datetime, features = 'dow') %>%
step_mutate(season = factor(season,
levels = c(1,2,3,4),
labels = c('Spring', 'Summer',
'Fall', 'Winter'))) %>%
step_rm(atemp, datetime)
# Create recipe for regression tree model
reg_tree_recipe <- recipe(count ~ ., data=bike_training_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(weather = factor(weather, levels = c(1,2,3))) %>%
step_time(datetime, features = 'hour') %>%
step_date(datetime, features = 'dow') %>%
step_mutate(season = factor(season,
levels = c(1,2,3,4),
labels = c('Spring', 'Summer',
'Fall', 'Winter'))) %>%
step_rm(atemp, datetime)
source("~/Library/Mobile Documents/com~apple~CloudDocs/BYU/Automne 2024/STAT 348/BikeShare/Bike_Reg_Tree.R", echo=TRUE)
# Create regression tree model
reg_tree <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n = tune()) %>%
set_engine('rpart') %>%
set_mode('regression')
# Create regression tree model
reg_tree_model <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n = tune()) %>%
set_engine('rpart') %>%
set_mode('regression')
# Set workflow
reg_tree_wf <- workflow() %>%
add_recipe(reg_tree_recipe) %>%
add_model(reg_tree_model)
# Grid of values to tune over
grid_tuning_params <- grid_regular(tree_depth(),
cost_complexity(),
min_n(),
levels = 5)
# Split data for cross validation (CV)
folds <- vfold_cv(bike_training_data, v = 10, repeats = 1)
# Load necessary libraries
library(tidyverse)
library(tidymodels)
library(vroom)
## Regression Tree ##
# Read in training data
bike_training_data <- vroom('train.csv')
# Read in test data
bike_test_data <- vroom('test.csv')
# Eliminate casual and registered columns from training data and
# change count to log(count)
bike_training_data <- bike_training_data %>%
select(-casual, -registered) %>%
mutate(count = log(count))
# Create recipe for regression tree model
reg_tree_recipe <- recipe(count ~ ., data=bike_training_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(weather = factor(weather, levels = c(1,2,3))) %>%
step_time(datetime, features = 'hour') %>%
step_date(datetime, features = 'dow') %>%
step_mutate(season = factor(season,
levels = c(1,2,3,4),
labels = c('Spring', 'Summer',
'Fall', 'Winter'))) %>%
step_rm(atemp, datetime)
# Create regression tree model
reg_tree_model <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n = tune()) %>%
set_engine('rpart') %>%
set_mode('regression')
# Set workflow
reg_tree_wf <- workflow() %>%
add_recipe(reg_tree_recipe) %>%
add_model(reg_tree_model)
# Grid of values to tune over
grid_tuning_params <- grid_regular(tree_depth(),
cost_complexity(),
min_n(),
levels = 5)
# Split data for cross validation (CV)
folds <- vfold_cv(bike_training_data, v = 10, repeats = 1)
# Run the CV
# Run the CV
cv_results <- reg_tree_wf %>%
tune_grid(resamples = folds,
grid = grid_tuning_params,
metrics = metric_set(rmse))
# Find best tuning params
best_params <- cv_results %>%
select_best(metric = 'rmse')
# Finalize workflow and fit it
final_wf <- reg_tree_wf %>%
finalize_workflow(best_params) %>%
fit(data = bike_training_data)
# Predict
reg_tree_preds <- predict(final_wf, new_data = bike_test_data) %>%
mutate(.pred = exp(.pred))
source("~/Library/Mobile Documents/com~apple~CloudDocs/BYU/Automne 2024/STAT 348/BikeShare/Bike_Reg_Tree.R", echo=TRUE)
# Format the Predictions for Submission to Kaggle
kaggle_submission <- tuned_penalized_preds %>%
bind_cols(., bike_test_data) %>% # Bind predictions with test data
select(datetime, .pred) %>% # Just keep datetime and prediction variables
rename(count = .pred) %>% # Rename to count (for submission to Kaggle)
mutate(count = pmax(0, count)) %>% # Pointwise max of (0, prediction)
mutate(datetime = as.character(format(datetime))) # Needed for right format to Kaggle
# Format the Predictions for Submission to Kaggle
kaggle_submission <- reg_tree_preds %>%
bind_cols(., bike_test_data) %>% # Bind predictions with test data
select(datetime, .pred) %>% # Just keep datetime and prediction variables
rename(count = .pred) %>% # Rename to count (for submission to Kaggle)
mutate(count = pmax(0, count)) %>% # Pointwise max of (0, prediction)
mutate(datetime = as.character(format(datetime))) # Needed for right format to Kaggle
# Write out the file
vroom_write(x = kaggle_submission, file = "./Regression_Tree.csv", delim = ",")

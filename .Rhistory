#change count to log(count)
bike_training_data <- bike_training_data %>%
select(-casual, -registered) %>%
mutate(count = log(count))
# Create recipe for penalized regression model
tuned_penalized_recipe <- recipe(count ~ ., data=bike_training_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(weather = factor(weather, levels = c(1,2,3))) %>%
step_time(datetime, features = 'hour') %>%
step_date(datetime, features = 'dow') %>%
step_mutate(season = factor(season,
levels = c(1,2,3,4),
labels = c('Spring', 'Summer',
'Fall', 'Winter'))) %>%
step_rm(atemp, datetime) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
# Tuned penalized regression model
tuned_penalized_model <- linear_reg(penalty = tune(),
mixture = tune()) %>%
set_engine('glmnet')
# Set Workflow
tuned_penalized_wf <- workflow() %>%
add_recipe(tuned_penalized_recipe) %>%
add_model(tuned_penalized_model)
# Grid of values to tune over
grid_tuning_params <- grid_regular(penalty(),
mixture(),
levels = 5)
# Split data for cross validation (CV)
folds <- vfold_cv(bike_training_data, v = 10, repeats = 1)
# Run the CV
cv_results <- tuned_penalized_wf %>%
tune_grid(resamples = folds,
grid = grid_tuning_params,
metrics = metric_set(rmse, mae, rsq))
# Plot results
collect_metrics(cv_results) %>%
filter(.metric == 'rmse') %>%
ggplot(data=., aes(x=penalty, y=mean, color=factor(mixture))) +
geom_line()
# Find best tuning params
best_params <- cv_results %>%
select_best(metric = 'rmse')
# Finalize workflow and fit it
final_wf <- tuned_penalized_wf %>%
finalize_workflow(best_params) %>%
fit(data = bike_training_data)
# Predict
tuned_penalized_preds <- predict(final_wf, new_data = bike_test_data) %>%
mutate(.pred = exp(.pred))
# Format the Predictions for Submission to Kaggle
kaggle_submission <- tuned_penalized_preds %>%
bind_cols(., bike_test_data) %>% # Bind predictions with test data
select(datetime, .pred) %>% # Just keep datetime and prediction variables
rename(count = .pred) %>% # Rename to count (for submission to Kaggle)
mutate(count = pmax(0, count)) %>% # Pointwise max of (0, prediction)
mutate(datetime = as.character(format(datetime))) # Needed for right format to Kaggle
# Load necessary libraries
library(tidyverse)
library(tidymodels)
library(DataExplorer)
library(vroom)
library(patchwork)
library(poissonreg)
## Tuned Penalized Model ##
# Read in training data
bike_training_data <- vroom('train.csv')
# Read in test data
bike_test_data <- vroom('test.csv')
# Eliminate casual and registered columns from training data and
#change count to log(count)
bike_training_data <- bike_training_data %>%
select(-casual, -registered) %>%
mutate(count = log(count))
# Create recipe for penalized regression model
tuned_penalized_recipe <- recipe(count ~ ., data=bike_training_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(weather = factor(weather, levels = c(1,2,3))) %>%
step_time(datetime, features = 'hour') %>%
step_date(datetime, features = 'dow') %>%
step_mutate(season = factor(season,
levels = c(1,2,3,4),
labels = c('Spring', 'Summer',
'Fall', 'Winter'))) %>%
step_rm(atemp, datetime) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
# Tuned penalized regression model
tuned_penalized_model <- linear_reg(penalty = tune(),
mixture = tune()) %>%
set_engine('glmnet')
# Set Workflow
tuned_penalized_wf <- workflow() %>%
add_recipe(tuned_penalized_recipe) %>%
add_model(tuned_penalized_model)
# Grid of values to tune over
grid_tuning_params <- grid_regular(penalty(),
mixture(),
levels = 5)
# Split data for cross validation (CV)
folds <- vfold_cv(bike_training_data, v = 10, repeats = 1)
# Run the CV
cv_results <- tuned_penalized_wf %>%
tune_grid(resamples = folds,
grid = grid_tuning_params,
metrics = metric_set(rmse, mae, rsq))
# Plot results
collect_metrics(cv_results) %>%
filter(.metric == 'rmse') %>%
ggplot(data=., aes(x=penalty, y=mean, color=factor(mixture))) +
geom_line()
# Find best tuning params
best_params <- cv_results %>%
select_best(metric = 'rmse')
# Finalize workflow and fit it
final_wf <- tuned_penalized_wf %>%
finalize_workflow(best_params) %>%
fit(data = bike_training_data)
# Predict
tuned_penalized_preds <- predict(final_wf, new_data = bike_test_data) %>%
mutate(.pred = exp(.pred))
# Format the Predictions for Submission to Kaggle
kaggle_submission <- tuned_penalized_preds %>%
bind_cols(., bike_test_data) %>% # Bind predictions with test data
select(datetime, .pred) %>% # Just keep datetime and prediction variables
rename(count = .pred) %>% # Rename to count (for submission to Kaggle)
mutate(count = pmax(0, count)) %>% # Pointwise max of (0, prediction)
mutate(datetime = as.character(format(datetime))) # Needed for right format to Kaggle
# Write out the file
vroom_write(x = kaggle_submission, file = "./Tuned_Penalized_Regression.csv", delim = ",")
# Load necessary libraries
library(tidyverse)
library(tidymodels)
library(vroom)
# Eliminate casual and registered columns from training data and
#change count to log(count)
bike_training_data <- bike_training_data %>%
select(-casual, -registered) %>%
mutate(count = log(count))
# Read in training data
bike_training_data <- vroom('train.csv')
# Read in test data
bike_test_data <- vroom('test.csv')
# Eliminate casual and registered columns from training data and
#change count to log(count)
bike_training_data <- bike_training_data %>%
select(-casual, -registered) %>%
mutate(count = log(count))
# Create recipe for regression tree model
tuned_penalized_recipe <- recipe(count ~ ., data=bike_training_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(weather = factor(weather, levels = c(1,2,3))) %>%
step_time(datetime, features = 'hour') %>%
step_date(datetime, features = 'dow') %>%
step_mutate(season = factor(season,
levels = c(1,2,3,4),
labels = c('Spring', 'Summer',
'Fall', 'Winter'))) %>%
step_rm(atemp, datetime)
# Create recipe for regression tree model
reg_tree_recipe <- recipe(count ~ ., data=bike_training_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(weather = factor(weather, levels = c(1,2,3))) %>%
step_time(datetime, features = 'hour') %>%
step_date(datetime, features = 'dow') %>%
step_mutate(season = factor(season,
levels = c(1,2,3,4),
labels = c('Spring', 'Summer',
'Fall', 'Winter'))) %>%
step_rm(atemp, datetime)
source("~/Library/Mobile Documents/com~apple~CloudDocs/BYU/Automne 2024/STAT 348/BikeShare/Bike_Reg_Tree.R", echo=TRUE)
# Create regression tree model
reg_tree <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n = tune()) %>%
set_engine('rpart') %>%
set_mode('regression')
# Create regression tree model
reg_tree_model <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n = tune()) %>%
set_engine('rpart') %>%
set_mode('regression')
# Set workflow
reg_tree_wf <- workflow() %>%
add_recipe(reg_tree_recipe) %>%
add_model(reg_tree_model)
# Grid of values to tune over
grid_tuning_params <- grid_regular(tree_depth(),
cost_complexity(),
min_n(),
levels = 5)
# Split data for cross validation (CV)
folds <- vfold_cv(bike_training_data, v = 10, repeats = 1)
# Load necessary libraries
library(tidyverse)
library(tidymodels)
library(vroom)
## Regression Tree ##
# Read in training data
bike_training_data <- vroom('train.csv')
# Read in test data
bike_test_data <- vroom('test.csv')
# Eliminate casual and registered columns from training data and
# change count to log(count)
bike_training_data <- bike_training_data %>%
select(-casual, -registered) %>%
mutate(count = log(count))
# Create recipe for regression tree model
reg_tree_recipe <- recipe(count ~ ., data=bike_training_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(weather = factor(weather, levels = c(1,2,3))) %>%
step_time(datetime, features = 'hour') %>%
step_date(datetime, features = 'dow') %>%
step_mutate(season = factor(season,
levels = c(1,2,3,4),
labels = c('Spring', 'Summer',
'Fall', 'Winter'))) %>%
step_rm(atemp, datetime)
# Create regression tree model
reg_tree_model <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n = tune()) %>%
set_engine('rpart') %>%
set_mode('regression')
# Set workflow
reg_tree_wf <- workflow() %>%
add_recipe(reg_tree_recipe) %>%
add_model(reg_tree_model)
# Grid of values to tune over
grid_tuning_params <- grid_regular(tree_depth(),
cost_complexity(),
min_n(),
levels = 5)
# Split data for cross validation (CV)
folds <- vfold_cv(bike_training_data, v = 10, repeats = 1)
# Run the CV
# Run the CV
cv_results <- reg_tree_wf %>%
tune_grid(resamples = folds,
grid = grid_tuning_params,
metrics = metric_set(rmse))
# Find best tuning params
best_params <- cv_results %>%
select_best(metric = 'rmse')
# Finalize workflow and fit it
final_wf <- reg_tree_wf %>%
finalize_workflow(best_params) %>%
fit(data = bike_training_data)
# Predict
reg_tree_preds <- predict(final_wf, new_data = bike_test_data) %>%
mutate(.pred = exp(.pred))
source("~/Library/Mobile Documents/com~apple~CloudDocs/BYU/Automne 2024/STAT 348/BikeShare/Bike_Reg_Tree.R", echo=TRUE)
# Format the Predictions for Submission to Kaggle
kaggle_submission <- tuned_penalized_preds %>%
bind_cols(., bike_test_data) %>% # Bind predictions with test data
select(datetime, .pred) %>% # Just keep datetime and prediction variables
rename(count = .pred) %>% # Rename to count (for submission to Kaggle)
mutate(count = pmax(0, count)) %>% # Pointwise max of (0, prediction)
mutate(datetime = as.character(format(datetime))) # Needed for right format to Kaggle
# Format the Predictions for Submission to Kaggle
kaggle_submission <- reg_tree_preds %>%
bind_cols(., bike_test_data) %>% # Bind predictions with test data
select(datetime, .pred) %>% # Just keep datetime and prediction variables
rename(count = .pred) %>% # Rename to count (for submission to Kaggle)
mutate(count = pmax(0, count)) %>% # Pointwise max of (0, prediction)
mutate(datetime = as.character(format(datetime))) # Needed for right format to Kaggle
# Write out the file
vroom_write(x = kaggle_submission, file = "./Regression_Tree.csv", delim = ",")
# Load necessary libraries
library(tidyverse)
library(tidymodels)
library(DataExplorer)
library(vroom)
library(patchwork)
library(poissonreg)
# Load necessary libraries
library(tidyverse)
library(tidymodels)
library(vroom)
# Read in training data
bike_training_data <- vroom('train.csv')
# Read in test data
bike_test_data <- vroom('test.csv')
# Eliminate casual and registered columns from training data and
# change count to log(count)
bike_training_data <- bike_training_data %>%
select(-casual, -registered) %>%
mutate(count = log(count))
# Create recipe for regression tree model
rand_forest_recipe <- recipe(count ~ ., data=bike_training_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(weather = factor(weather, levels = c(1,2,3))) %>%
step_time(datetime, features = 'hour') %>%
step_date(datetime, features = 'dow') %>%
step_mutate(season = factor(season,
levels = c(1,2,3,4),
labels = c('Spring', 'Summer',
'Fall', 'Winter'))) %>%
step_rm(atemp, datetime)
# Create random forest model
rand_forest_model <- rand_forest(mtry = tune(),
min_n = tune(),
trees = 1000) %>%
set_engine('ranger') %>%
set_mode('regression')
# Set workflow
rand_forest_model <- workflow() %>%
add_recipe(rand_forest_recipe) %>%
add_model(rand_forest_model)
# Grid of values to tune over
rand_forest_grid_params <- grid_regular(mtry(range = c(1, ncol(bake(rand_forest_recipe, bike_training_data)))),
min_n(),
levels = 5)
prep(rand_forest_recipe) %>% ncol(bake(rand_forest_recipe, bike_training_data))
prep(rand_forest_recipe) %>% bake(bike_training_data) %>% ncol()
# Grid of values to tune over
prep(rand_forest_recipe) %>% bake(bike_training_data) %>% ncol() # = 10
rand_forest_grid_params <- grid_regular(mtry(range = c(1, 10)),
min_n(),
levels = 5)
# Split data for cross validation
folds <- vfold_cv(bike_training_data, v = 10, repeats = 1)
# Create random forest model
rand_forest_model <- rand_forest(mtry = tune(),
min_n = tune(),
trees = 1000) %>%
set_engine('ranger') %>%
set_mode('regression')
# Set workflow
rand_forest_wf <- workflow() %>%
add_recipe(rand_forest_recipe) %>%
add_model(rand_forest_model)
# Grid of values to tune over
prep(rand_forest_recipe) %>% bake(bike_training_data) %>% ncol() # = 10
rand_forest_grid_params <- grid_regular(mtry(range = c(1, 10)),
min_n(),
levels = 5)
# Split data for cross validation
folds <- vfold_cv(bike_training_data, v = 10, repeats = 1)
# Run the cv
cv_results <- rand_for
# Run the cv
cv_results <- rand_forest_wf %>%
tune_grid(resamples = folds,
grid = rand_forest_grid_params,
metrics = metric_set(rmse))
install.packages("ranger")
# Run the cv
cv_results <- rand_forest_wf %>%
tune_grid(resamples = folds,
grid = rand_forest_grid_params,
metrics = metric_set(rmse))
# Find best tuning params
best_params <- cv_results %>%
select_best(metric = 'rmse')
# Finalize workflow and fit it
final_wf <- rand_forest_wf %>%
finalize_workflow(best_params) %>%
fit(data = bike_training_data)
# Predict
rand_forest_preds <- predict(final_wf, new_data = bike_test_data) %>%
mutate(.pred = exp(.pred))
# Format the Predictions for Submission to Kaggle
kaggle_submission <- rand_forest_preds %>%
bind_cols(., bike_test_data) %>% # Bind predictions with test data
select(datetime, .pred) %>% # Just keep datetime and prediction variables
rename(count = .pred) %>% # Rename to count (for submission to Kaggle)
mutate(count = pmax(0, count)) %>% # Pointwise max of (0, prediction)
mutate(datetime = as.character(format(datetime))) # Needed for right format to Kaggle
# Write out the file
vroom_write(x = kaggle_submission, file = "./Random_Forest.csv", delim = ",")
install.packages("stacks")
# Load necessary libraries
library(tidyverse)
library(tidymodels)
library(vroom)
library(stacks)
# Load necessary libraries
library(tidyverse)
library(tidymodels)
library(vroom)
library(stacks)
## Stacked Model ##
# Read in training data
bike_training_data <- vroom('train.csv')
# Read in test data
bike_test_data <- vroom('test.csv')
# Eliminate casual and registered columns from training data and
# change count to log(count)
bike_training_data <- bike_training_data %>%
select(-casual, -registered) %>%
mutate(count = log(count))
# Create recipe for regression tree model
rand_forest_recipe <- recipe(count ~ ., data=bike_training_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(weather = factor(weather, levels = c(1,2,3))) %>%
step_time(datetime, features = 'hour') %>%
step_date(datetime, features = 'dow') %>%
step_mutate(season = factor(season,
levels = c(1,2,3,4),
labels = c('Spring', 'Summer',
'Fall', 'Winter'))) %>%
step_rm(atemp, datetime)
# Split data for cv
folds <- vfold_cv(bike_training_data, v = 10, repeats = 1)
# Create a control grid
untuned_model <- control_stack_grid()
# Penalized regression model
penalized_reg_model <- linear_reg(penalty = tune(),
mixture = tune()) %>%
set_engine('glmnet')
# Load necessary libraries
library(tidyverse)
library(tidymodels)
library(vroom)
library(stacks)
## Stacked Model ##
# Read in training data
bike_training_data <- vroom('train.csv')
# Read in test data
bike_test_data <- vroom('test.csv')
# Eliminate casual and registered columns from training data and
# change count to log(count)
bike_training_data <- bike_training_data %>%
select(-casual, -registered) %>%
mutate(count = log(count))
# Create recipe for regression tree model
model_recipe <- recipe(count ~ ., data=bike_training_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(weather = factor(weather, levels = c(1,2,3))) %>%
step_time(datetime, features = 'hour') %>%
step_date(datetime, features = 'dow') %>%
step_mutate(season = factor(season,
levels = c(1,2,3,4),
labels = c('Spring', 'Summer',
'Fall', 'Winter'))) %>%
step_rm(atemp, datetime)
# Split data for cv
folds <- vfold_cv(bike_training_data, v = 10, repeats = 1)
# Create a control grid
untuned_model <- control_stack_grid()
# Penalized regression model
penalized_reg_model <- linear_reg(penalty = tune(),
mixture = tune()) %>%
set_engine('glmnet')
# Set workflow
penalized_reg_wf <- workflow() %>%
add_recipe(model_recipe) %>%
add_model(penalized_reg_model)
# Grid of values to tune over
penalized_reg_grid_params <- grid_regular(penalty(),
mixture(),
levels = 5)
# Run the CV
penalized_red_models <- penalized_reg_wf %>%
tune_grid(resamples = folds,
grid = penalized_reg_grid_params,
metrics = metric_set(rmse),
control = untuned_model)
# Load necessary libraries
library(tidyverse)
library(tidymodels)
library(vroom)
library(stacks)
## Stacked Model ##
# Read in training data
bike_training_data <- vroom('train.csv')
# Read in test data
bike_test_data <- vroom('test.csv')
# Eliminate casual and registered columns from training data and
# change count to log(count)
bike_training_data <- bike_training_data %>%
select(-casual, -registered) %>%
mutate(count = log(count))
# Create recipe for regression tree model
model_recipe <- recipe(count ~ ., data=bike_training_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(weather = factor(weather, levels = c(1,2,3))) %>%
step_time(datetime, features = 'hour') %>%
step_date(datetime, features = 'dow') %>%
step_mutate(season = factor(season,
levels = c(1,2,3,4),
labels = c('Spring', 'Summer',
'Fall', 'Winter'))) %>%
step_rm(atemp, datetime) |>
step_dummy(all_nominal_predictors()) |>
step_normalize(all_numeric_predictors())
# Split data for cv
folds <- vfold_cv(bike_training_data, v = 10, repeats = 1)
# Create a control grid
untuned_model <- control_stack_grid()
# Penalized regression model
penalized_reg_model <- linear_reg(penalty = tune(),
mixture = tune()) %>%
set_engine('glmnet')
# Set workflow
penalized_reg_wf <- workflow() %>%
add_recipe(model_recipe) %>%
add_model(penalized_reg_model)
# Grid of values to tune over
penalized_reg_grid_params <- grid_regular(penalty(),
mixture(),
levels = 5)
# Run the CV
penalized_red_models <- penalized_reg_wf %>%
tune_grid(resamples = folds,
grid = penalized_reg_grid_params,
metrics = metric_set(rmse),
control = untuned_model)
untuned_model
